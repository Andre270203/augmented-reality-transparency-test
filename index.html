<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Back Home AR Experience</title>

  <script src="https://cdn.jsdelivr.net/gh/aframevr/aframe@1.6.0/dist/aframe-master.min.js"></script>
  <script src="https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js"></script>

  <style>
    body { margin: 0; overflow: hidden; font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; }
    .arjs-loader {
      height: 100%; width: 100%; position: absolute; top: 0; left: 0;
      background: rgba(0,0,0,0.8); z-index: 9999; display: flex; justify-content: center; align-items: center;
      color: #fff; font-size: 1.1em;
    }
    .tap-overlay {
      position: absolute; inset: 0; display: none; z-index: 10000;
      background: rgba(0,0,0,0.55); color: #fff; font-weight: 700;
      align-items: center; justify-content: center; text-align: center; padding: 24px;
    }
    .tap-btn {
      margin-top: 12px; background: #4caf50; color: #fff; border: 0; border-radius: 20px; padding: 10px 18px; font-weight: 700;
    }
  </style>

  <script>
    const isIOS = /iPad|iPhone|iPod/.test(navigator.userAgent);
    const isAndroid = /Android/i.test(navigator.userAgent);

    // Simple videoplayer (your working Android logic) + iOS user-gesture gate
    AFRAME.registerComponent('videoplayer', {
      init: function () {
        const videoColor = document.querySelector('#videoColor');
        const videoAlpha = document.querySelector('#videoAlpha');

        // --- iOS: require a user gesture to unmute+play color video ---
        let iosReady = !isIOS;    // Android/desktop: ready immediately; iOS: wait for tap
        const tapOverlay = document.getElementById('tapOverlay');
        const tapBtn = document.getElementById('tapBtn');

        if (isIOS) {
          // On iOS we start both videos muted; we only unmute color after a tap.
          // Ensure attributes exist (some Safari versions are picky).
          videoColor.setAttribute('muted', '');
          videoAlpha.setAttribute('muted', '');

          // Show overlay until user taps
          tapOverlay.style.display = 'flex';

          const enableIOS = () => {
            // Try to start both videos muted first (satisfy autoplay policy),
            // then unmute color and resume.
            const startMuted = Promise.all([
              videoColor.play().catch(() => {}),
              videoAlpha.play().catch(() => {})
            ]).then(() => {
              videoColor.muted = false;  // allow audio after gesture
              // Calling play again makes Safari commit to rendering the video frames to the texture
              return videoColor.play().catch(() => {});
            }).finally(() => {
              iosReady = true;
              tapOverlay.style.display = 'none';
            });

            // Remove extra listeners after first tap
            tapBtn.removeEventListener('click', enableIOS);
            tapOverlay.removeEventListener('click', enableIOS);
          };

          tapBtn.addEventListener('click', enableIOS, { once: true });
          tapOverlay.addEventListener('click', enableIOS, { once: true });
        }

        // Your original marker handlers, with a small guard for iOS readiness
        this.el.addEventListener('markerFound', function () {
          // On Android this just plays; on iOS it plays only after tap has happened.
          if (videoColor.paused && iosReady) { videoColor.play().catch(()=>{}); }
          if (videoAlpha.paused && iosReady) { videoAlpha.play().catch(()=>{}); }
        });

        this.el.addEventListener('markerLost', function () {
          if (!videoColor.paused) videoColor.pause();
          if (!videoAlpha.paused) videoAlpha.pause();
        });
      }
    });
  </script>
</head>

<body style="margin: 0; overflow: hidden;">
  <div class="arjs-loader" id="loader"><div>Loading, please waitâ€¦</div></div>

  <!-- iOS tap-to-start overlay -->
  <div class="tap-overlay" id="tapOverlay">
    <div>
      <div>Tap to enable video on iOS</div>
      <button class="tap-btn" id="tapBtn">Enable</button>
    </div>
  </div>

  <a-scene
    vr-mode-ui="enabled: false;"
    renderer="logarithmicDepthBuffer: true; precision: medium;"
    embedded
    arjs="sourceType: webcam; trackingMethod: best; debugUIEnabled: false;"
    onloaded="document.getElementById('loader').style.display='none';"
  >
    <a-assets>
      <!-- Use the same sources you had. Key differences:
           - Keep playsinline/webkit-playsinline for iOS
           - Add muted by default; iOS will unmute color after user gesture in code above
           - Remove 'autoplay' for maximum iOS reliability; Android still starts on markerFound
      -->
      <video
        id="videoColor"
        src="https://raw.githack.com/Andre270203/augmented-reality-transparency-test/main/AR_Video01_Color.mp4"
        preload="metadata"
        loop
        muted
        crossorigin="anonymous"
        webkit-playsinline
        playsinline
      ></video>

      <video
        id="videoAlpha"
        src="https://raw.githack.com/Andre270203/augmented-reality-transparency-test/main/AR_Video01_Alpha.mp4"
        preload="metadata"
        loop
        muted
        crossorigin="anonymous"
        webkit-playsinline
        playsinline
      ></video>
    </a-assets>

    <!-- Shader (unchanged) -->
    <script id="matte-shader" type="x-shader/x-fragment">
      precision mediump float;
      uniform sampler2D videoColor;
      uniform sampler2D videoAlpha;
      varying vec2 vUV;
      void main() {
        vec4 color = texture2D(videoColor, vUV);
        vec4 alpha = texture2D(videoAlpha, vUV);
        gl_FragColor = vec4(color.rgb, alpha.r);
      }
    </script>

    <script>
      AFRAME.registerShader('video-matte-shader', {
        schema: {
          videoColor: { type: 'map', is: 'uniform' },
          videoAlpha: { type: 'map', is: 'uniform' }
        },
        fragmentShader: document.getElementById('matte-shader').textContent,
        vertexShader: `
          varying vec2 vUV;
          void main() {
            vUV = uv;
            gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
          }
        `
      });
    </script>

    <!-- Marker + plane -->
    <a-nft
      videoplayer
      type="nft"
      url="https://raw.githack.com/Andre270203/augmented-reality-transparency-test/main/marker/AR_Image-1"
      smooth="true"
      smoothCount="10"
      smoothTolerance="0.01"
      smoothThreshold="5"
    >
      <a-entity
        geometry="primitive: plane; width: 300; height: 175"
        material="shader: video-matte-shader;
                  videoColor: #videoColor;
                  videoAlpha: #videoAlpha;
                  transparent: true;"
        position="50 150 -100"
        rotation="-90 0 0"
      ></a-entity>
    </a-nft>

    <a-entity camera look-controls position="0 1.6 0"></a-entity>
  </a-scene>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Back Home AR</title>

  <script src="https://aframe.io/releases/1.6.0/aframe.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mind-ar@1.2.5/dist/mindar-image-aframe.prod.js"></script>

  <style>
    html, body { margin:0; height:100%; overflow:hidden; background:transparent; }
    #audio-prompt {
      position: fixed;
      top: 20px;
      left: 50%;
      transform: translateX(-50%);
      background: rgba(0,0,0,0.8);
      color: white;
      padding: 10px 20px;
      border-radius: 5px;
      font-family: Arial, sans-serif;
      z-index: 1000;
      display: none;
    }
  </style>

  <script>
    AFRAME.registerShader('video-matte', {
      schema: {
        videoColor: { type: 'map', is: 'uniform' },
        videoAlpha: { type: 'map', is: 'uniform' },
        opacity:    { type: 'number', default: 1.0, is: 'uniform' }
      },
      vertexShader: `varying vec2 vUV; void main(){ vUV=uv; gl_Position=projectionMatrix*modelViewMatrix*vec4(position,1.0); }`,
      fragmentShader: `
        precision highp float; varying vec2 vUV;
        uniform sampler2D videoColor, videoAlpha; uniform float opacity;
        void main(){
          vec4 c = texture2D(videoColor, vUV);
          float a = texture2D(videoAlpha, vUV).r;
          a = smoothstep(0.15, 0.95, a) * opacity;
          gl_FragColor = vec4(c.rgb, a);
        }
      `
    });
  </script>
</head>

<body>
  <div id="audio-prompt">Tap anywhere to enable audio</div>
  
  <a-scene
    mindar-image="imageTargetSrc: https://raw.githack.com/alevalve/augmented-reality-transparency-test/main/targets.mind; autoStart: true"
    color-space="sRGB"
    renderer="colorManagement: true; physicallyCorrectLights: true"
    vr-mode-ui="enabled: false"
    device-orientation-permission-ui="enabled: true"
    embedded
  >
    <a-assets>
      <video id="videoColor"
             src="https://raw.githack.com/alevalve/augmented-reality-transparency-test/main/AR_Video01_Color.mp4"
             loop muted playsinline webkit-playsinline crossorigin="anonymous"></video>
      <video id="videoAlpha"
             src="https://raw.githack.com/alevalve/augmented-reality-transparency-test/main/AR_Video01_Alpha.mp4"
             loop muted playsinline webkit-playsinline crossorigin="anonymous"></video>
    </a-assets>

    <a-camera position="0 0 0" look-controls="enabled: false"></a-camera>

    <a-entity id="marker" mindar-image-target="targetIndex: 0">
      <a-plane id="videoPlane"
        width="0.9" height="0.55"
        rotation="0 0 0" position="0 0 0"
        visible="false"
        material="shader: video-matte; videoColor: #videoColor; videoAlpha: #videoAlpha; transparent: true; depthWrite: false">
      </a-plane>
    </a-entity>
  </a-scene>

  <script>
    const marker = document.getElementById('marker');
    const plane  = document.getElementById('videoPlane');
    const vc     = document.getElementById('videoColor');
    const va     = document.getElementById('videoAlpha');
    const audioPrompt = document.getElementById('audio-prompt');

    let markerVisible = false;
    let audioEnabled = false;
    let userHasInteracted = false;

    // Initialize videos as muted
    vc.muted = true; 
    va.muted = true;

    function bothRendering() {
      return vc.readyState >= 2 && va.readyState >= 2;
    }

    // Simplified audio enablement
    async function enableAudio() {
      if (audioEnabled) return;
      
      try {
        // Unmute both videos (in case audio is in either one)
        vc.muted = false;
        va.muted = false;
        
        // Try to play both videos with audio
        await vc.play();
        await va.play();
        
        audioEnabled = true;
        audioPrompt.style.display = 'none';
        console.log('Audio enabled successfully');
      } catch (error) {
        console.log('Audio enable failed:', error);
        // Keep videos muted if audio fails
        vc.muted = true;
        va.muted = true;
      }
    }

    // Global click/touch handler for audio enablement
    function setupAudioUnlock() {
      const unlockAudio = async (e) => {
        userHasInteracted = true;
        if (markerVisible && !audioEnabled) {
          await enableAudio();
        }
        // Remove listeners after first interaction
        document.removeEventListener('click', unlockAudio);
        document.removeEventListener('touchstart', unlockAudio);
      };

      document.addEventListener('click', unlockAudio);
      document.addEventListener('touchstart', unlockAudio);
    }

    // Set up audio unlock on page load
    setupAudioUnlock();

    marker.addEventListener('targetFound', async () => {
      markerVisible = true;
      plane.setAttribute('visible', true);

      // Start playback (muted initially)
      try { 
        await vc.play(); 
        await va.play();
      } catch (error) {
        console.log('Video play failed:', error);
      }

      // Show audio prompt if user hasn't interacted yet
      if (!userHasInteracted && !audioEnabled) {
        audioPrompt.style.display = 'block';
      }

      // If user has already interacted, try to enable audio immediately
      if (userHasInteracted && !audioEnabled) {
        await enableAudio();
      }
    });

    marker.addEventListener('targetLost', () => {
      markerVisible = false;
      plane.setAttribute('visible', false);
      audioPrompt.style.display = 'none';
      
      // Pause videos
      vc.pause(); 
      va.pause();
      
      // Reset to beginning
      vc.currentTime = 0; 
      va.currentTime = 0;
      
      // Re-mute for next time (required for autoplay)
      vc.muted = true;
      va.muted = true;
      audioEnabled = false;
    });

    // Handle video loading errors
    vc.addEventListener('error', (e) => console.log('Color video error:', e));
    va.addEventListener('error', (e) => console.log('Alpha video error:', e));
  </script>
</body>
</html>